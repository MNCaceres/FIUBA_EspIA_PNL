![Ejemplo de Imagen](/img/2.png)

Este repositorio contiene el desarrollo de los desafíos realizados en el curso de **Procesamiento del Lenguaje Natural (PLN)** de la Especialización en Inteligencia Artificial de la FIUBA.



<div align="center" style="margin-top: 60px; margin-bottom: 60px;">
  <img src="/img/2_2.png" alt="Texto alternativo" width="700"/>
</div>


## Descripción del curso 

El curso está organizado en 8 clases, donde se abordan los principales conceptos y técnicas de PLN, cubriendo desde los fundamentos hasta los modelos más avanzados utilizados en la actualidad. Los temas se desarrollan a través de 5 desafíos, en los cuales se aplican las técnicas estudiadas para resolver problemas concretos en PLN.

## Temario del curso

### Clase 1: Introducción a NLP, Vectorización de documentos
- Conceptos fundamentales de Procesamiento del Lenguaje Natural.
- Representación de texto mediante técnicas de vectorización como Bag of Words (BoW) y TF-IDF.

### Clase 2: Pre-procesamiento de texto. Word embeddings
- Limpieza y normalización de texto.
- Introducción a word embeddings como Word2Vec, GloVe y FastText, y su importancia en la representación distribuida de palabras.

### Clase 3: Modelos no-BOW I: convolucionales y recurrentes. Generación de secuencias
- Modelos alternativos a BoW para capturar relaciones en secuencias.
- Redes neuronales convolucionales (CNNs) aplicadas a secuencias.
- Redes neuronales recurrentes (RNNs), LSTMs y GRUs.
- Introducción a la generación de texto secuencial.

### Clase 4: Modelos no-BOW II: mecanismo de atención
- Introducción al mecanismo de atención para mejorar el rendimiento en modelos secuenciales.
- Aplicaciones de atención en modelos de PLN.

### Clase 5: Modelos Seq2seq
- Arquitectura de encoder-decoder para tareas de secuencias a secuencias.
- Aplicaciones como traducción automática, chatbots y resumen de texto.

### Clase 6: Transformers
- Arquitectura de Transformers, incluyendo el modelo de atención completa.
- Modelos como BERT, GPT, y su impacto en PLN.

### Clase 7: Grandes modelos de lenguaje. RAG
- Introducción a grandes modelos de lenguaje (LLMs).
- Modelos de recuperación-augmented generation (RAG) para mejorar la generación de respuestas en diálogos.

### Clase 8: Otros temas: Image captioning. ASR y TTS
- Generación de descripciones automáticas de imágenes (Image Captioning).
- Reconocimiento automático del habla (ASR).
- Síntesis de texto a voz (TTS).

## Estructura del curso

A lo largo del curso, se realizaron 5 desafíos que abarcan los principales temas vistos en clase:

1. **Desafío 1:** Vectorización de texto y preprocesamiento.
2. **Desafío 2:** Implementación de embeddings y modelos no-BOW.
3. **Desafío 3:** Modelos secuenciales con atención.
4. **Desafío 4:** Implementación de 2 modelos Seq2Seq: char - word.
5. **Desafío 5:** Aplicaciones avanzadas con Transformers y LLMs.

Cada desafío incluye el código implementado, y los resultados obtenidos.

## Requisitos

- Python 3.x
- Bibliotecas: TensorFlow, Keras, Scikit-learn, Numpy, etc.

## Instrucciones de uso

1. Clonar el repositorio:
    ```bash
    git clone https://github.com/MNCaceres/FIUBA_EspIA_PNL
    ```

2. Instalar dependencias:
    ```bash
    pip install -r requirements.txt
    ```

3. Ejecutar los notebooks de cada desafío para ver las implementaciones.
